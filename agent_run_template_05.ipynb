{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.env_2players_naive_torus import NaiveSandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss.losses import sum_loss_l1\n",
    "from model.a2c import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_to_cells(probs, env, topk=3, player=1):\n",
    "    probs_top_k, idx_top_k = probs.topk(topk)\n",
    "    inserted_block = np.zeros(env.shape)\n",
    "    inserted_block = inserted_block.flatten()\n",
    "    inserted_block[idx_top_k] = player\n",
    "    return inserted_block.reshape(env.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(perception_field, env_state, render_agent=False):\n",
    "    clear_output(wait=True)\n",
    "    if render_agent:\n",
    "        plt.imshow(perception_field)\n",
    "        plt.show()\n",
    "    #print('field after agent inference')\n",
    "    plt.imshow(env_state['grid'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward.rewards import AliveCellsReward, MultipleAgentsCellsReward\n",
    "reward_fn = MultipleAgentsCellsReward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(old, new):\n",
    "    if old == 0:\n",
    "        return new\n",
    "    return old\n",
    "merge_perceptions = np.vectorize(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_agents(first_probs, first_values, second_probs, second_values):\n",
    "    probs = [first_probs, second_probs]\n",
    "    if first_values != second_values:\n",
    "        if first_values == 0:\n",
    "            return second_values\n",
    "        if second_values == 0:\n",
    "            return first_values\n",
    "        #return probs.index(max(probs)) + 1\n",
    "        return 0\n",
    "    return first_values\n",
    "merge_agents = np.vectorize(merge_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def flatten_grid(grid):\n",
    "    return torch.tensor(grid).float().flatten().to(DEVICE)\n",
    "\n",
    "def unsqueeze_grid(grid):\n",
    "    return torch.FloatTensor(grid).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "def unsqueeze_states(states):\n",
    "    return torch.FloatTensor(states).view(-1, 10, 10).unsqueeze(1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_agent(agent, agent_id=1):\n",
    "    probs = agent.get_action_probs(flatten_grid(sandbox.get_grid()))\n",
    "    perception_field = probs_to_cells(probs.detach(), sandbox.get_grid(), N_CELLS, player=agent_id)\n",
    "    return perception_field, probs.view(sandbox.get_grid().shape).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def play_game(iterations, state, render_env=True, fall_asleep=False):\n",
    "    for i in range(iterations):\n",
    "        perception_1, probs_1 = step_agent(agent_1, 1)\n",
    "        perception_2, probs_2 = step_agent(agent_2, 2)\n",
    "        \n",
    "        perception_field = merge_agents(probs_1, perception_1, probs_2, perception_2)\n",
    "        perception_field = merge_perceptions(sandbox.get_grid(), perception_field)\n",
    "        \n",
    "        sandbox.insert_block(perception_field, 0, 0)\n",
    "        for s in range(steps_after_action):\n",
    "            if render_env:\n",
    "                render(perception_1, {'grid':sandbox.get_grid()})\n",
    "            if fall_asleep:\n",
    "                sleep(1.5)\n",
    "            sandbox.step()\n",
    "\n",
    "            rewards = reward_fn(sandbox.get_grid())\n",
    "            env_state = {\"grid\": sandbox.get_grid(), \"rewards\":  rewards, \"done\": sandbox.is_done()}\n",
    "\n",
    "            if render_env:\n",
    "                render(perception_1, env_state)\n",
    "            next_state, reward, done = env_state['grid'], env_state['rewards'], env_state['done']\n",
    "\n",
    "            states.append(state.flatten());dones.append(done);\n",
    "            \n",
    "            intristic_reward = rnd.get_intristic_reward(unsqueeze_grid(state))\n",
    "            \n",
    "            rewards1.append(intristic_reward.detach() + torch.tensor(reward[0]).to(DEVICE))\n",
    "            rewards2.append(intristic_reward.detach() + torch.tensor(reward[1]).to(DEVICE))\n",
    "            actions1.append(perception_1) \n",
    "            actions2.append(perception_2)\n",
    "            \n",
    "\n",
    "            state = next_state\n",
    "            if fall_asleep: sleep(0.5)\n",
    "        if fall_asleep:\n",
    "            print (\"Agents step NOW\")\n",
    "            sleep(0.5)\n",
    "    print (np.mean(rewards1), np.mean(rewards2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_agent(agent, optimizer, states, actions, rewards, dones, list_loss):\n",
    "    \n",
    "    intristic_reward = rnd.get_intristic_reward(unsqueeze_states(states))\n",
    "    intristic_reward.backward()\n",
    "    rnd_optimizer.step()\n",
    "    \n",
    "    total_loss = agent.reflect(states, actions, rewards, dones)\n",
    "    if np.isnan(total_loss.item()):\n",
    "        return\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    list_loss.append(total_loss.item())\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, y0 = 0, 0\n",
    "iterations = 100\n",
    "steps_after_action = 10\n",
    "N_GAMES = 100\n",
    "N_CELLS = 10\n",
    "GRID = 10\n",
    "LR = 0.001\n",
    "\n",
    "grid_size=(GRID,GRID)\n",
    "sandbox = NaiveSandbox(grid_size=grid_size)\n",
    "state = sandbox.get_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward.curiosity_reward import RNDModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = ActorCritic(GRID*GRID, GRID*GRID).to(DEVICE)\n",
    "agent_2 = ActorCritic(GRID*GRID, GRID*GRID).to(DEVICE)\n",
    "\n",
    "\n",
    "rnd = RNDModel(1, 1).to(DEVICE)\n",
    "rnd_optimizer = optim.Adam(rnd.parameters())\n",
    "\n",
    "optimizer_1 = optim.Adam(agent_1.parameters(), lr=LR)\n",
    "optimizer_2 = optim.Adam(agent_2.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_loss1, list_loss2 = [], []\n",
    "reward_list1, reward_list2 = [], []\n",
    "for game in range(N_GAMES):\n",
    "    states, actions1, actions2, rewards1, rewards2, dones = [], [], [], [], [], []\n",
    "    \n",
    "    play_game(iterations, state, render_env=False)\n",
    "    reflect_agent(agent_1, optimizer_1, states, actions1, rewards1, dones, list_loss1)\n",
    "    reflect_agent(agent_2, optimizer_2, states, actions2, rewards2, dones, list_loss2)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    reward_list1.append(np.mean(rewards1))\n",
    "    reward_list2.append(np.mean(rewards2))\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(list(range(len(list_loss1))), list_loss1)\n",
    "    plt.plot(list(range(len(list_loss2))), list_loss2)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(list(range(len(reward_list1))), reward_list1)\n",
    "    plt.plot(list(range(len(reward_list2))), reward_list2)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-7966e76c0430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msteps_after_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfall_asleep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-e79f34f80b41>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(iterations, state, render_env, fall_asleep)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperception_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'grid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfall_asleep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_size=(GRID,GRID)\n",
    "sandbox = NaiveSandbox(grid_size=grid_size)\n",
    "state = sandbox.get_grid()\n",
    "from time import sleep\n",
    "steps_after_action = 10\n",
    "states, actions, rewards, dones = [], [], [], []\n",
    "play_game(30, state, fall_asleep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
