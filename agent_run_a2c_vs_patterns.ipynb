{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from pylab import rcParams\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.players.bot import BotPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.env_2players_naive_torus import NaiveSandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss.losses import sum_loss_l1\n",
    "from model.a2c import ActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(perception_field=None, env_state=None, render_agent=False):\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env_state['grid'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward.rewards import AliveCellsReward, MultipleAgentsCellsReward, MultipleAgentsCellsRewardWithEnemyDiscount\n",
    "reward_fn = MultipleAgentsCellsReward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_agents(first_values,second_values):\n",
    "    if all([first_values, second_values]):\n",
    "        return 0\n",
    "    return second_values if first_values == 0 else first_values\n",
    "merge_agents = np.vectorize(merge_agents)\n",
    "\n",
    "def merge_perceptions(old, new):\n",
    "    return new if old == 0 else old\n",
    "merge_perceptions = np.vectorize(merge_perceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def flatten_grid(grid):\n",
    "    return torch.tensor(grid).float().flatten().to(DEVICE)\n",
    "\n",
    "def unsqueeze_grid(grid):\n",
    "    return torch.FloatTensor(grid).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "def unsqueeze_states(states):\n",
    "    return torch.FloatTensor(states).view(-1, GRID, GRID).unsqueeze(1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def play_game(iterations, state, render_env=True, fall_asleep=False):\n",
    "    for i in range(iterations):\n",
    "        perception_2, actions_2 = agent_2.step(flatten_grid(sandbox.get_grid()))\n",
    "        \n",
    "        perception_field = perception_2\n",
    "        perception_field = merge_perceptions(sandbox.get_grid(), perception_field)\n",
    "        \n",
    "        sandbox.insert_block(perception_field, 0, 0)\n",
    "        \n",
    "        reward_signal_weight = 0.1\n",
    "        \n",
    "        for s in range(steps_after_action):\n",
    "            if render_env:\n",
    "                render(perception_2, {'grid':sandbox.get_grid()})\n",
    "            if fall_asleep:\n",
    "                sleep(1.5)\n",
    "            sandbox.step()\n",
    "\n",
    "            rewards = reward_fn(sandbox.get_grid())\n",
    "            env_state = {\"grid\": sandbox.get_grid(), \"rewards\":  rewards, \"done\": sandbox.is_done()}\n",
    "\n",
    "            if render_env:\n",
    "                render(None, env_state)\n",
    "            next_state, reward, done = env_state['grid'], env_state['rewards'], env_state['done']\n",
    "\n",
    "            states.append(state.flatten());dones.append(done);\n",
    "            \n",
    "            intristic_reward = rnd.get_intristic_reward(unsqueeze_grid(state))\n",
    "            total_reward_2 = intristic_reward.detach() + torch.tensor(reward[1]) * reward_signal_weight\n",
    "            \n",
    "            rewards2.append(total_reward_2)\n",
    "            actions2.append(actions_2)\n",
    "            \n",
    "            reward_signal_weight += 0.1\n",
    "            state = next_state\n",
    "            if fall_asleep: sleep(0.5)\n",
    "        if fall_asleep:\n",
    "            print (\"Agents step NOW\")\n",
    "            sleep(0.5)\n",
    "    print (np.mean(rewards2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_agent(agent, optimizer, states, actions, rewards, dones, list_loss):\n",
    "    rnd_optimizer.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    intristic_reward = rnd.get_intristic_reward(unsqueeze_states(states))\n",
    "    \n",
    "    intristic_reward.backward()\n",
    "    rnd_optimizer.step()\n",
    "    \n",
    "    total_loss = agent.reflect(states, actions, rewards, dones)\n",
    "    if np.isnan(total_loss.item()):\n",
    "        return\n",
    "    \n",
    "    total_loss.backward()\n",
    "    \n",
    "    list_loss.append(total_loss.item())\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, y0 = 0, 0\n",
    "iterations = 200\n",
    "steps_after_action = 10\n",
    "N_GAMES = 10\n",
    "N_CELLS = 10\n",
    "GRID = 50\n",
    "LR = 0.001\n",
    "\n",
    "grid_size=(GRID,GRID)\n",
    "sandbox = NaiveSandbox(grid_size=grid_size)\n",
    "state = sandbox.get_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward.curiosity_reward import RNDModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2 = BotPlayer(sandbox, marker=2, max_points_per_step=9, perception_field_size=(4, 4), hard_x_y=(40,40))\n",
    "\n",
    "\n",
    "rnd = RNDModel(GRID).to(DEVICE)\n",
    "rnd_optimizer = optim.Adam(rnd.parameters())\n",
    "\n",
    "optimizer_2 = optim.Adam(agent_2.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(agent, epoch):\n",
    "    torch.save(agent.state_dict(), './snapshots/agent_vs_gun_{}.pth'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patterns.gliders import gosper_glider_gun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_loss1, list_loss2 = [], []\n",
    "reward_list1, reward_list2 = [], []\n",
    "for game in range(N_GAMES):\n",
    "    \n",
    "    grid_size=(GRID,GRID)\n",
    "    sandbox = NaiveSandbox(grid_size=grid_size)\n",
    "    sandbox.insert_block(gosper_glider_gun(), 20, 0)\n",
    "    state = sandbox.get_grid()\n",
    "    \n",
    "    states, actions1, actions2, rewards1, rewards2, dones = [], [], [], [], [], []\n",
    "    \n",
    "    play_game(iterations, state, render_env=False)\n",
    "    reflect_agent(agent_2, optimizer_2, states, actions2, rewards2, dones, list_loss2)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    reward_list2.append(np.mean(rewards2))\n",
    "    \n",
    "    if game % 10 == 0:\n",
    "        save(agent_2, epoch=game)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(list(range(len(list_loss2))), list_loss2)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(list(range(len(reward_list2))), reward_list2)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAC75JREFUeJzt3X+o3fV9x/HnazE/+mOi6VSyRKaFUCxjVQhWcH8UrSyzpfqHA10ZGQTyzwaWFdp0g7HC/tB/av8Zg1Cl+aNUu7agSKGEVBmDEY0/2mmDTSpsDQazUYPtYKlp3/vjfpW7eK/35Nxzzj1n7+cDLud8v/d77vfF9b78fD+f+825qSok9fJbGx1A0uxZfKkhiy81ZPGlhiy+1JDFlxqy+FJDFl9qaF3FT7I3yStJTiU5OKlQkqYr4965l2QT8BPgDuA08CxwX1X9eLXXbMnW2sYHxjqfpLX9D//Nr+p81jrusnWc42bgVFW9CpDkUeAuYNXib+MDfDy3r+OUkt7LsTo60nHrudTfCfxs2fbpYZ+kObeeEX+ly4l3zRuSHAAOAGzj/es4naRJWU/xTwPXLtveBbx28UFVdQg4BHB5tvtPAefc91978T0//0e/e+OMkmia1nOp/yywO8n1SbYA9wJPTCaWpGkae8SvqgtJ/hL4PrAJeKSqXp5YMklTs55Lfarqe8D3JpRF0ox4557UkMWXGrL4UkMWX2rI4ksNrWtVv5uLb26Z1M0s0/q64xjn3PN00888ZZlnjvhSQxZfasjiSw05xx+sNDd0PjgZfm/njyO+1JDFlxqy+FJDFl9qaEMX99a62QKmd5PMOOdxQWo8s/y++d9oNI74UkMWX2rI4ksNeQPPHJjEvHRS6yXjrIU4r148jvhSQxZfasjiSw1ZfKmhDV3cc1FoZeMs1K30vRzl66gnR3ypIYsvNWTxpYba3MCz1nrCKO8SM0/vhjutf+DkuksPjvhSQxZfasjiSw21meOvZZ7mtuP8Y5p5yq/554gvNWTxpYbWLH6SR5KcTfLSsn3bkxxJcnJ4vHK6MSVN0igj/teBvRftOwgcrardwNFhW9KCSFWtfVByHfBkVf3+sP0K8ImqOpNkB/B0VX1kra9zebbXx3P7+hJLWtWxOsqb9fOsddy4c/xrquoMwPB49ZhfR9IGmPqv85IcAA4AbOP90z6dpBGMO+K/PlziMzyeXe3AqjpUVXuqas9mto55OkmTNG7xnwD2Dc/3AY9PJo6kWRjl13nfBP4V+EiS00n2Aw8AdyQ5CdwxbEtaEGvO8avqvlU+5fK8tKC8c09qyOJLDVl8qSGLLzVk8aWGLL7UkO/Ao0uyaO9GrJU54ksNWXypIYsvNeQcX5fEv8r7/4MjvtSQxZcasvhSQ87xNXH+3n7+OeJLDVl8qSGLLzVk8aWGXNzTurmYt3gc8aWGLL7UkMWXGrL4UkMWX2rI4ksNWXypIYsvNWTxpYYsvtSQxZcasvhSQxZfasjiSw1ZfKmhNYuf5NokTyU5keTlJPcP+7cnOZLk5PB45fTjSpqEUd6I4wLw+ap6PslvA88lOQL8OXC0qh5IchA4CHxxelGl/2uUv9yrla054lfVmap6fnj+C+AEsBO4Czg8HHYYuHtaISVN1iXN8ZNcB9wEHAOuqaozsPQ/B+DqSYeTNB0jFz/JB4HvAJ+rqjcv4XUHkhxPcvwtzo+TUdKEjVT8JJtZKv03quq7w+7Xk+wYPr8DOLvSa6vqUFXtqao9m9k6icyS1mnNxb0kAR4GTlTVV5Z96glgH/DA8Pj4VBJKqxjnT3a7+LdklFX9W4E/A/4tydvf1b9mqfDfSrIf+A/gT6YTUdKkrVn8qvoXIKt8+vbJxpE0C965JzXkX9LRwlprPg/O6VfjiC81ZPGlhiy+1JBzfC0s5+/jc8SXGrL4UkMWX2rI4ksNWXypIYsvNWTxpYYsvtSQxZcasvhSQxZfasjiSw1ZfKkhiy81ZPGlhiy+1JDFlxqy+FJDFl9qyOJLDVl8qSHfZVcaw8V/xefid/xd6a/8zNO7AjviSw1ZfKkhiy815BxfGsMoc/p55ogvNWTxpYbWLH6SbUmeSfLDJC8n+fKw//okx5KcTPJYki3TjytpEkYZ8c8Dt1XVx4Abgb1JbgEeBB6qqt3AG8D+6cWUNElrLu5VVQG/HDY3Dx8F3Ab86bD/MPB3wD9OPqK0eObpZp2VjDTHT7IpyYvAWeAI8FPgXFVdGA45DeycTkRJkzZS8avq11V1I7ALuBm4YaXDVnptkgNJjic5/hbnx08qaWIuaVW/qs4BTwO3AFckeXuqsAt4bZXXHKqqPVW1ZzNb15NV0oSsOcdPchXwVlWdS/I+4JMsLew9BdwDPArsAx6fZlBpns37nP5io9y5twM4nGQTS1cI36qqJ5P8GHg0yd8DLwAPTzGnpAkaZVX/R8BNK+x/laX5vqQF4517UkMWX2rI4ksNWXypIYsvNWTxpYYsvtSQxZcasvhSQxZfasjiSw1ZfKkhiy81ZPGlhiy+1JDFlxqy+FJDFl9qyOJLDVl8qSGLLzVk8aWGLL7UkMWXGrL4UkMWX2rI4ksNWXypIYsvNWTxpYYsvtSQxZcasvhSQxZfasjiSw2NXPwkm5K8kOTJYfv6JMeSnEzyWJIt04spaZIuZcS/HzixbPtB4KGq2g28AeyfZDBJ0zNS8ZPsAj4FfG3YDnAb8O3hkMPA3dMIKGnyRh3xvwp8AfjNsP0h4FxVXRi2TwM7V3phkgNJjic5/hbn1xVW0mSsWfwknwbOVtVzy3evcGit9PqqOlRVe6pqz2a2jhlT0iRdNsIxtwKfSXInsA24nKUrgCuSXDaM+ruA16YXU9IkrTniV9WXqmpXVV0H3Av8oKo+CzwF3DMctg94fGopJU3Uen6P/0Xgr5KcYmnO//BkIkmatlEu9d9RVU8DTw/PXwVunnwkSdPmnXtSQxZfasjiSw1ZfKkhiy81ZPGlhiy+1JDFlxqy+FJDFl9qyOJLDVl8qSGLLzVk8aWGLL7UkMWXGrL4UkMWX2rI4ksNWXypIYsvNWTxpYYsvtSQxZcasvhSQxZfasjiSw1ZfKkhiy81ZPGlhiy+1JDFlxqy+FJDFl9qyOJLDVl8qaFU1exOlvwn8O/A7wD/NbMTr88iZYXFyrtIWWEx8v5eVV211kEzLf47J02OV9WemZ94DIuUFRYr7yJlhcXL+1681JcasvhSQxtV/EMbdN5xLFJWWKy8i5QVFi/vqjZkji9pY3mpLzU00+In2ZvklSSnkhyc5blHkeSRJGeTvLRs3/YkR5KcHB6v3MiMb0tybZKnkpxI8nKS+4f985p3W5JnkvxwyPvlYf/1SY4NeR9LsmWjs74tyaYkLyR5ctie26yXambFT7IJ+Afgj4GPAvcl+eiszj+irwN7L9p3EDhaVbuBo8P2PLgAfL6qbgBuAf5i+H7Oa97zwG1V9THgRmBvkluAB4GHhrxvAPs3MOPF7gdOLNue56yXZJYj/s3Aqap6tap+BTwK3DXD86+pqv4Z+PlFu+8CDg/PDwN3zzTUKqrqTFU9Pzz/BUs/oDuZ37xVVb8cNjcPHwXcBnx72D83eZPsAj4FfG3YDnOadRyzLP5O4GfLtk8P++bdNVV1BpbKBly9wXneJcl1wE3AMeY473Dp/CJwFjgC/BQ4V1UXhkPm6Wfiq8AXgN8M2x9ifrNeslkWPyvs81cK65Tkg8B3gM9V1Zsbnee9VNWvq+pGYBdLV4A3rHTYbFO9W5JPA2er6rnlu1c4dMOzjuuyGZ7rNHDtsu1dwGszPP+4Xk+yo6rOJNnB0mg1F5JsZqn036iq7w675zbv26rqXJKnWVqbuCLJZcNIOi8/E7cCn0lyJ7ANuJylK4B5zDqWWY74zwK7h5XRLcC9wBMzPP+4ngD2Dc/3AY9vYJZ3DHPOh4ETVfWVZZ+a17xXJblieP4+4JMsrUs8BdwzHDYXeavqS1W1q6quY+nn9AdV9VnmMOvYqmpmH8CdwE9Ymtv9zSzPPWK+bwJngLdYukLZz9Lc7ihwcnjcvtE5h6x/yNKl5o+AF4ePO+c47x8ALwx5XwL+dtj/YeAZ4BTwT8DWjc56Ue5PAE8uQtZL+fDOPakh79yTGrL4UkMWX2rI4ksNWXypIYsvNWTxpYYsvtTQ/wKhYXGQTNPWjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sandbox = NaiveSandbox(grid_size=(50, 50))\n",
    "sandbox.insert_block(gosper_glider_gun(), 0, 0)\n",
    "render(env_state={'grid': sandbox.get_grid()})\n",
    "for i in range(100):\n",
    "    sandbox.step()\n",
    "    render(env_state={'grid': sandbox.get_grid()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 2500], m2: [400 x 64] at /opt/conda/conda-bld/pytorch-cpu_1532576596369/work/aten/src/TH/generic/THTensorMath.cpp:2070",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-68018d496cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfall_asleep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-63abf4a612e4>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(iterations, state, render_env, fall_asleep)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfall_asleep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mperception_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mperception_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/research/GoL-RL/games/players/bot.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mperception_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/research/GoL-RL/model/a2c.py\u001b[0m in \u001b[0;36mget_action_probs\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Only the Actor head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/research/GoL-RL/model/a2c.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# In a PyTorch model, you only have to define the forward pass. PyTorch computes the backwards pass for you!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 2500], m2: [400 x 64] at /opt/conda/conda-bld/pytorch-cpu_1532576596369/work/aten/src/TH/generic/THTensorMath.cpp:2070"
     ]
    }
   ],
   "source": [
    "grid_size=(GRID,GRID)\n",
    "sandbox = NaiveSandbox(grid_size=grid_size)\n",
    "sandbox.insert_block(gosper_glider_gun(), 20, 0)\n",
    "state = sandbox.get_grid()\n",
    "from time import sleep\n",
    "states, actions, rewards, dones = [], [], [], []\n",
    "play_game(50, state, fall_asleep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_cells, first_player, second_player = np.array([12, 9, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_player * (first_player - second_player) / dead_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_player * (second_player - first_player) / dead_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
